snippet tfargparse "Argparse" b
parser = argparse.ArgumentParser(description="${1:description}")

parser.add_argument('--use_gpu',
					type=int,
					required=True,
					help='Specify the gpu to use')

parser.add_argument('-gpuf',
					'--gpu_fraction',
					type=float,
					default=1.0,
					help='Fraction of GPU memory to be allocated')

parser.add_argument('-agg',
					'--allow_gpu_growth',
					action="store_true",
					help='Allow GPU memory growth, when needed')

parser.add_argument('--seed',
					type=int,
					default=1123,
					help='Default seed to set')

parser.add_argument('--log_path',
					type=str,
					required=True,
					help='Log Path')

parser.add_argument('--use_dropout',
                    action="store_true",
                    help='Use dropout in each rnn cell')

parser.add_argument('-dprate',
                    '--dropout_rate'
                    type=float,
                    default=0.3,
                    help='Dropout probability for input/output/state units (0.0: no dropout)')

parser.add_argument('-lr',
                    '--learning_rate',
                    type=float,
                    default=0.0002,
                    help='Learning rate')

parser.add_argument('--max_gradient_norm',
                    type=float,
                    default=1.0,
                    help='Clip gradients to this norm')

parser.add_argument('--batch_size',
                    type=int,
                    default=128,
                    help='Batch size')

parser.add_argument('--max_epochs',
                    type=int,
                    default=10,
                    help='Maximum # of training epochs')

parser.add_argument('-opt',
                    '--optimizer',
                    type=str,
                    default='adam',
                    help='Optimizer for training: (adadelta, adam, rmsprop)')

args = parser.parse_args()
endsnippet

snippet addargs "Add parser argument" b
parser.add_argument('--${1:var_name}',
					type=${2:type},
					action=${3:"store_true"},
					required=${4:True},
					help='${5:help}')
endsnippet

snippet tfimp "Tensorflow essential imports" b
import numpy as np
import tensorflow as tf
import os, glob, gzip, time
import logging
import argparse

from datetime import datetime
from collections import OrderedDict
from collections import namedtuple

np.random.seed(123)
endsnippet

snippet gpuenviron "Set GPU environment" b
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]=str(${1:0}))
endsnippet

snippet tfmain "Main function for tensorflow" b
def main():
	gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=${1:1.0})
	config = tf.ConfigProto(gpu_options=gpu_options)
	config.gpu_options.allow_growth = ${2:True}

	saver = tf.train.Saver(max_to_keep=None)
	merged = tf.summary.merge_all()
	val_writer = tf.summary.FileWriter(os.path.join(${3:log_dir}, "val"))
	train_writer = tf.summary.FileWriter(os.path.join(${3:log_dir}, "train"))

	with tf.Session(config=config) as sess:
		sess.run(tf.global_variables_initializer())
		train_writer.add_graph(sess.graph)

		print "TRAINING STARTED..."

		for i in range(${4:epochs}):
			batch = ${5:Dataset.next_batch}


			train_writer.add_summary()
			save_file_path = os.path.join(args.log_path, "checkpoint", "${6:model_name}")
			saver.save(sess, save_file_path, global_step = ${7:i})

if __name__ == "__main__":
	main()
endsnippet

snippet tfArch "Tensorflow Architecture basic" b
class ${1:ARCHITECTURE}:

	OPTIMIZERS = {
			"adam":tf.train.AdamOptimizer,
			"adadelta":tf.train.AdadeltaOptimizer,
			"rmsprop":tf.train.RMSPropOptimizer,
			"sgd":tf.train.GradientDescentOptimizer
	}

	def __init__(self, config):
		# Fill this
		self.build_model()

	def initialize_placeholders(self):
		pass

	def build_model(self):
		logger.info("Building Model ...")
		self.initialize_placeholder()
		pass

	def init_loss(self):
		pass

	def init_optimizer(self):
		logger.info("Setting optimizer")
		trainable_params = tf.trainable_variables()
		self.opt = self.optimizer(learning_rate=self.learning_rate)

		grads = tf.gradients(self.loss, trainable_params)
		clip_grads, _ = tf.clip_by_global_norm(grads, self.max_grad_norm)

		self.updates = self.opt.apply_gradients(zip(clip_grads, trainable_params),global_step=self.global_step)

	def save(self, sess, path, var_list=None, global_step=None):
		saver = tf.train.Saver(var_list)
		save_path = saver.save(sess, save_path=path, global_step=global_step)
		logger.info("Model saved at {}".format(save_path))

	def restore(self, sess, path, var_list=None):
		saver = tf.train.Saver(var_list)
		saver.restore(sess, save_path=path)
		logger.info("Model restored from {}".format(path))

	def train(self, sess, ${2:inputs, outputs}):
		input_feed = self.check_feeds($2)
		output_feed = [${3:outputs}]
		# Keep prob dropout
		outputs = sess.run(output_feed, input_feed)
		return outputs

	def eval(self, sess, ${4:inputs, outputs}):
		input_feed = self.check_feeds($4)
		output_feed = [${5:outputs}]
		# No dropout here
		outputs = sess.run(output_feed, input_feed)
		return outputs

	def predict(self, sess, ${6:inputs}):
		input_feed = self.check_feeds($6)
		output_feed = [${7:logits}]
		# No dropout
		outputs = sess.run(output_feed, input_feed)
		return outputs

	def check_feeds(self, ${8:inputs}):
		# Sanity check for input data like shape, etc.
		input_feed = {}

		# Fill input_feed
		input_feed[${9:Someplaceholder.name}] = ${10:inputs}

		return input_feed
endsnippet

snippet tfplace "Placeholder" b
${1:Var_name} = tf.placeholder(tf.${2:float32}, (None, ${3:300}), name="$1")
endsnippet

snippet tfns "Namescope" b
with tf.name_scope("${1:name}"):
	${2:something_here"}
endsnippet

snippet tfsc "Scalar summary" b
tf.summary.scalar("${1:loss}", $1)
endsnippet

snippet tfhist "Histogram summary" b
tf.summary.histogram(${1:parameter}, $1)
endsnippet

snippet tfvar "tf Variable" b
${1:W} = tf.Variable(${2:tf.random_normal()}, ${3:start_end}, name="W_{${4:1}}")
endsnippet

snippet tfrun "session runner" b
${1:vars} = sess.run([${2:fetch_var}], feed_dict={${3:feed_dictionary}})
endsnippet

snippet tfsumadd "Add summary" b
${1:writer_file}.add_summary(summary,${2:i})
endsnippet

snippet loggert "Add logger statements" b
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(name)s | %(message)s")

file_handler = logging.FileHandler(os.path.join(${1:args.logfolder}, ${2:args.logfile}))
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(stream_handler)
endsnippet

snippet tfimpseq2seq "Imports for sequence to sequence" b
import tensorflow as tf
import tensorflow.contrib.seq2seq as seq2seq

from tensorflow.python.ops.rnn_cell import GRUCell
from tensorflow.python.ops.rnn_cell import LSTMCell
from tensorflow.python.ops.rnn_cell import MultiRNNCell
from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper

from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.layers.core import Dense
from tensorflow.python.util import nest

from tensorflow.contrib.seq2seq.python.ops import attention_wrapper
from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder
endsnippet

snippet jobl "Joblib functionality" b
from joblib import Parallel, delayed

Parallel(n_jobs=${1:20})(delayed(${2:func_name}) for ${3:i} in range(${4:list_name}))
endsnippet

snippet tfmaintrain "Main run file for tensorflow architectures" b
import os, math, logging
import time, json
import random, argparse
from datetime import datetime
from pprint import pprint

from collections import OrderedDict

import numpy as np
import tensorflow as tf

from ..models.${1:model_file_name} import ${2:model_name}

def build_parser():
	parser = argparse.ArgumentParser(description="Run model")

	# Mode specifications
    parser.add_argument("--mode",
                        type=str, default="train", choices=['train', 'decode'],
                        help="Modes: train, decode")
    parser.add_argument("--dataset",
                        type=str, default="quora", choices=['quora', 'mscoco', 'twitter'],
                        help="Dataset to use")
    parser.add_argument("--run_name",
                        type=str, required=True,
                        help="Enter the run name")

	# Device Configuration
    parser.add_argument('--use_gpu',
                        type=int, required=True,
                        help='Specify the gpu to use')
    parser.add_argument('-gpuf', '--gpu_fraction',
                        type=float, default=1.0,
                        help='Fraction of GPU memory to be allocated')
    parser.add_argument('-agg', '--allow_gpu_growth',
                        action="store_true",
                        help='Allow GPU memory growth, when needed')
    parser.add_argument('--seed',
                        type=int, default=1123,
                        help='Default seed to set')
    parser.add_argument('--log_path',
                        type=str, required=True,
                        help='Log Path')
    parser.add_argument('--allow_soft_placement',
                        action="store_true",
                        help='Allow device soft placement')
    parser.add_argument('--log_device_placement',
                        action="store_true",
                        help='Log placement of ops on devices')
    parser.add_argument('--model_dir',
                        type=str, default='model/',
                        help='Path to save model checkpoints')
    parser.add_argument("--log_folder",
                        type=str, default="Logs",
                        help="Logger path to capture the stream outputs")
    parser.add_argument("-log_fmt", "--logger_format",
                        type=str, default="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
                        help="Specify format of the logger")
    parser.add_argument("--log_file",
                        type=str, default="log_",
                        help="Log file name")
    parser.add_argument("-date_fmt", "--dateformat",
                        type=str, default="%Y-%m-%d_%H:%M:%S",
                        help="Format of the date")

	# Training parameters
    parser.add_argument('-shuffle', '--shuffle_each_epoch',
                        action="store_true",
                        help='Shuffle training dataset for each epoch')
    parser.add_argument('-sbl', '--sort_by_length',
                        action="store_true",
                        help='Sort pre-fetched minibatches by their target sequence lengths')
    parser.add_argument('-use_dp', '--use_dropout',
                        action="store_true",
                        help='Use dropout in each rnn cell')
    parser.add_argument('-dprate', '--dropout_rate',
                        type=float, default=0.3,
                        help='Dropout probability for input/output/state units (0.0: no dropout)')
    parser.add_argument('-lr', '--learning_rate',
                        type=float, default=0.0002,
                        help='Learning rate')
    parser.add_argument('--max_gradient_norm',
                        type=float, default=1.0,
                        help='Clip gradients to this norm')
    parser.add_argument('--batch_size',
                        type=int, default=32,
                        help='Batch size')
    parser.add_argument('--max_epochs',
                        type=int, default=10,
                        help='Maximum # of training epochs')
    parser.add_argument('-opt', '--optimizer',
                        type=str, default='adam', choices=['adam', 'adadelta', 'sgd', 'rmsprop'],
                        help='Optimizer for training: (adadelta, adam, rmsprop, sgd)')

	return parser

def read_files(args):
    raise NotImplementedError("Read files not implemented")

def create_model(session, args, logger):
    config  = args
    logger.info(json.dumps(vars(args),
                           sort_keys=True,
                           indent=4))

    model   = ${3:model_name}(config, args.mode, logger)

    # ckpt = tf.train.get_checkpoint_state(args.model_dir)
    ckpt = tf.train.get_checkpoint_state(os.path.join(args.model_dir,
                                                      args.run_name,
                                                      'train',
                                                      'checkpoint'))

    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
        logger.info('Reloading model parameters..')
        model.restore(session, ckpt.model_checkpoint_path)
    else:
        if not os.path.exists(args.model_dir):
            os.makedirs(args.model_dir)
        logger.info('Created new model parameters..')
        session.run(tf.global_variables_initializer())

    return model

def validation(args, model, sess, i, logger):
	raise NotImplementedError("Validation code not implemented")

def fit(args, model, sess, i, logger):
	raise NotImplementedError("Validation code not implemented")

def start_function(args, logger):
	# Load parallel data to train
    logger.info('Loading training data..')

    # Gpu Configurations
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_fraction)
    gpu_config  = tf.ConfigProto(gpu_options=gpu_options)
    gpu_config.gpu_options.allow_growth = args.allow_gpu_growth

	seq2seq_cvae_model_path = os.path.join(args.model_dir, args.run_name)
    train_writer = tf.summary.FileWriter(os.path.join(seq2seq_cvae_model_path, "train"))
    val_writer = tf.summary.FileWriter(os.path.join(seq2seq_cvae_model_path, "val"))

    with tf.Session(config=gpu_config) as sess:
        model = create_model(sess, args, logger)

        if model.global_epoch_step.eval() >= args.max_epochs:
            logger.info("training complete curr_epoch: {},max_epoch:{}".format(
                model.global_epoch_step.eval(), args.max_epochs))
            sys.exit()

        train_writer.add_graph(sess.graph)
        val_writer.add_graph(sess.graph)

        logger.info("TRAINING STARTED...")
        min_loss = 1e8
        ebnum = 0

        for i in range(1, args.max_epochs+1):

            epoch_loss_train = fit(args, model, sess, i, logger, train_data, w2id, id2w, train_writer)
            save_file_path = os.path.join(seq2seq_cvae_model_path,
                                          'train',
                                          "checkpoint",
                                          'autoencoder')

            # Validation check
            epoch_loss_val = validation(args, model, sess, i, logger, dev_data, w2id, id2w, val_writer)


            # Save Model only when less than min_loss
            if epoch_loss_val < min_loss:
                model.save(sess, save_file_path, global_step=model.global_epoch_step)
                min_loss = epoch_loss_val;
            else:
                logger.info('Not saving model since eval loss exceeded min loss')
            model.global_epoch_step_op.eval()


            if model.global_epoch_step.eval() >= args.max_epochs:
                logger.info("Training complete curr_epoch: {} max_epoch:{}".format(model.global_epoch_step.eval(),
                                                                                   args.max_epochs))
                sys.exit()


def main():
    parser = build_parser()
    args = parser.parse_args()

    np.random.seed(args.seed)
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.use_gpu)

 	log_folder_name = os.path.join(args.log_folder, args.run_name)
    if not os.path.exists(log_folder_name):
        os.makedirs(log_folder_name)

    logger = get_logger(__name__,
                        args.logger_format,
                        logging.INFO,
                        os.path.join(log_folder_name,
                                     'autoencoder.log'))

    start_function(args, logger)

if __name__ == "__main__":
	tf.app.run()
endsnippet
